---
title: "ML Assignment"
author: "Rithvik"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This document describes use of ML models for prediction of exercise type in a given dataset called the Weight Lifting Exercise Dataset. The data was collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. http://groupware.les.inf.puc-rio.br/har
For this purpose, we need to load the dataset, train an ML model and use the model to predict the exercise method for the test data.

# Data loading and cleaning

## Download data

The data is downloaded and loaded as `maindata` and `outsample` using read.csv and default options.
```{r}
urlpath <- "https://d396qusza40orc.cloudfront.net/predmachlearn/"
maindata <- read.csv(paste(urlpath,"pml-training.csv", sep=""))
outsample <- read.csv(paste(urlpath,"pml-testing.csv",sep=""))
```

## Clean data

We can use `View()` in RStudio to check the data. `dim(maindata)=``r dim(maindata)` and `dim(outsample)=``r dim(outsample)`. The number of columns is the same, however maindata has a `classe` column, while outsample has a dummy variable `problem_id` which is a counter 1:20.
We note that many columns are having NA values, except when the `new_window` column has value `yes`. In the latter case, there are values instead of NA. The outsample data has no `new_window` values given as `yes`. Thus it is better to filter out all entries where `new_window=="yes"`. The number of entries with `new_window=="yes"` is very small, i.e.`dim(maindata[maindata$new_window=="yes",])=``r dim(maindata[maindata$new_window=="yes",])` compared to `r dim(maindata[maindata$new_window=="no",])[1]` entries for `new_window=="no"`. Since the function of the `new_window` column is not clear, we will use the `new_window=="no"` entries only.
```{r}
maindata <- maindata[maindata$new_window=="no",]
```

This makes several columns with NA values. These can be removed immediately from both maindata and outsample. We will check that no extra columns are removed and the maindata and sample match. 

```{r}
contains_any_na = sapply(outsample, function(x) any(is.na(x)))
maindata <- maindata[,!contains_any_na]
outsample <- outsample[,!contains_any_na]
```
The first 6 columns are removed and preserved in another variable in case we need it later. The same procedure is used on both the maindata and outsample.
```{r}
maindata_1st6 <- maindata[,c(1:6)] 
maindata <- maindata[,-c(1:6)]
```
```{r}
outsample_1st6 <- outsample[,c(1:6)]
outsample <- outsample[,-c(1:6)]
```
We can make factor variable out of the classe column.
```{r}
maindata$classe <- as.factor(maindata$classe)
```
The new dataset is now defined as dim(maindata)=`r dim(maindata)`. And the outsample is dim(outsample)=`r dim(outsample)`. Thus we have 54 variables in both. There are 19216 entries in the maindata for training.

# Machine learning

We first load the library needed

```{r message=FALSE}
library(caret)
```

First we will split the maindata into a training and test set, so we can choose the best machine learning strategy.

```{r}
inTrain <- createDataPartition(maindata$classe, p = 3/4)[[1]]
training <- maindata[inTrain,]
testing <- maindata[-inTrain,]
```
Using this strategy we can try many methods

```{r rpartfit,cache=TRUE}
rpartFit <- train(classe ~ ., data = training, method = "rpart")
```
```{r}
pred_rpart <- predict(rpartFit, newdata = testing[,-55], type = "raw")
```

```{r}
cMpred_rpart <- confusionMatrix(data=pred_rpart, reference=testing$classe)
cMpred_rpart$table
cMpred_rpart$overall
```
Clearly the Classification and Regression Trees (CART) model is not very accurate with `r cMpred_rpart$overall[1]*100`% accuracy by default. It may work with some tweaking. Other methods like "glm" or "lm" do not work as the number of variables is very large, or in case of "glm", the output levels are too many. 

Boost methods combine many different small regressors using different strategies. One commonly used boost method which requires very little tweaking from default is the "gbm" or Gradient boosting machine. We can use this technique as it is perfect for the use-case we have.

```{r gbmfit,cache=TRUE}
gbmFit <- train(classe ~ ., data = training, 
                 method = "gbm",
                 verbose = FALSE)
```

```{r}
pred_gbm <- predict(gbmFit, newdata = testing[,-55], type = "raw")
```
```{r}
cMpred_gbm <- confusionMatrix(data=pred_gbm, reference=testing$classe)
cMpred_gbm$table
cMpred_gbm$overall[1:2]
```
We can see that the training confusionmatrix gives `r cMpred_gbm$overall[1]*100`% accuracy. This is much better than "rpart". Thus we can use the "gbm" model for our final analysis

# Cross Validation

There are many methods for cross validation. One we have already used, by partitioning the data into training and testing sets. However, we can use more rigorous methods on the whole maindata so that we can have a model which we can then use on the outsample. We can use "cv" or "repeatcv" or k-fold cross validations. We can use K=10 and 10 repeats of the same with "repeatcv", and then take the average to remove any bias in sampling.

```{r}
fitControl <- trainControl( method = "repeatedcv",number = 10, repeats = 10)
set.seed(1729)
```

The seed can be anything, including the [taxicab number](https://en.wikipedia.org/wiki/Taxicab_number).

```{r gbmfit1,cache=TRUE}
gbmFit1 <- train(classe ~ ., data = maindata, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE)
```

Now we can test the model. Since the outsample does not give expected outcome, we can use the testing set we had before. **HOWEVER**, this is **inaccurate**, as the testing set is part of the maindata set which is the training set for the model. Thus we can expect the numbers to be better than the previous case. We can check it out anyway:

```{r eval=TRUE}
pred_gbm1 <- predict(gbmFit1, newdata = testing[, -55], type = "raw")
```
```{r}
cMpred_gbm1 <- confusionMatrix(data=pred_gbm1, reference=testing$classe)
cMpred_gbm1$table
cMpred_gbm1$overall[1:2]
```
The `r cMpred_gbm1$overall[1]*100`% accuracy is much better, but this is expected as the training set contained the test set, so this increase is really meaningless.
The model itself can be checked by its plots
```{r fig.cap="We can see the increase in estimated accuracy as the boosting iterations are increased" }
ggplot(gbmFit1)
```

# Application of the model

The model can now be applied to our 'outsample'
```{r eval=TRUE}
pred_prob <- predict(gbmFit1, newdata = outsample, type = "prob")
pred_prob
```
We can see some columns having $p>0.6$, which are the predictions of the model.
```{r}
pred_mat <- pred_prob>0.6
pred_mat
```
 And the predictions themselves can be easily obtained using `type = "raw"`
 
# Conclusion

The final prediction is given in the **classe** column below:
```{r}
pred_final <- predict(gbmFit1, newdata = outsample, type = "raw")
cbind(outsample_1st6[,2:3],classe=pred_final)
```

Thanks!!


